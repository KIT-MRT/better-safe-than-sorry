\section{Safe Arbitration Graphs}

Designing autonomous systems ideally involves ensuring that each behavior component operates reliably under all potential conditions.
However, achieving this level of reliability is often impractical, especially when deploying these systems in complex and dynamic environments.
Real-world applications might involve numerous behaviors, implemented by different teams and using various methods.
With increasing complexity, the likelihood of bugs and inconsistencies grows, making it challenging to ensure the overall safety and robustness of such systems.

The responsibility for safety in autonomous platforms is a critical consideration.
Typically, each developer is responsible for their behavior component.
Additionally, when using hierarchical approaches such as \glspl{BT},
    a system engineer is in charge of integrating these behavior components into a coherent system,
    bearing the overall responsibility for operational safety.
This responsibility is manageable when components are simple and isolated.
However, it does not scale well with increasingly complex architectures.

To address these challenges, we propose a safety concept embedded directly into the arbitration graph framework.
With this approach, we aim to reduce the burden on both behavior component developers and system engineers
by shifting the responsibility for the platform's safety to the verification step within the decision-making framework.
This ensures that even in the presence of unreliable or unsafe behavior components, the system as a whole remains robust and safe.

In this section, we will explore methods for detecting and mitigating unsafe and unreliable behavior components, with the following safety goals in mind:

\begin{itemize}
    \item The system ensures that a safety action is taken when operating conditions exceed its designed capabilities.
    \item The system is robust against failures of behavior components.
    \item The system prevents invalid behavior commands.
    \item The system avoids risky behavior commands.
\end{itemize}

\subsection{\textbf{Detect} Unsafe \& Unreliable Behaviors}
\begin{algorithm}
  \SetKwFunction{BestOption}{BestOption}%
  \SetKwProg{Fn}{function}{}{end}

  \Fn{\BestOption{situation $\situation$}}{\label{algo:verifying_arbitrator:bestOption}
    filter applicable options $\applicableOptions \subseteq \options$\;
    sort applicable options
      $\sortedApplicableOptions = \left< a_0, a_1, \dots \right> = \text{strategie}(\applicableOptions)$\;

    \For{$a \in \sortedApplicableOptions$}{
      get command $\command_a = \getCommand_a(\situation)$\;

      verify $\verification_a = \verifier (\command_a)$\;
      \If{verification passed $\verification_a = 0$}{%
        \KwRet{$(\command_a, \verification_a)$}\;
      }
    }

    \KwRet{$(\emptyset, \text{NO\_SAFE\_OPTION})$}\;
    }
  \;
  \While{true}{
    determine current situation $\situation$\;\label{algo:verifying_arbitrator:situation}
    determine $\verifiedCommand = (\command, \verification) = $ \BestOption{$\situation$}\;
    \If{verification passed $\verification_a = 0$}{
      execute $\command$\;
    }
  }
  \caption{Generic arbitration with verification \label{algo:verifying_arbitrator}}
\end{algorithm}

\Cref{algo:verifying_arbitrator} shows the generic arbitration algorithm from \cite{lauerCognitiveConceptsAutonomous2010}
that we have extended with a verification logic.
The verifier~$\verifier$ is domain-specific and may run various checks on the behavior command.
% Examples of verifiers
In Pac-Man for example, the verifier could check whether the command would lead to a collision with walls or ghosts.
In autonomous driving, one could verify if the behavior output is free of collisions and respects traffic rules.
Similarly, a robotic manipulator could verify if the command is within the workspace of the robot and does not exceed joint limits.
More generally, the verifier could check if the format of the command is correct and respects the specification of the system.
% Multiple verifiers
Integrating the verification step into the arbitration algorithm allows for the use of different verifiers at various levels within the arbitration graph.
This enables a more fine-grained and adaptable safety concept.
For example, verifiers can be customized for specific scenarios, or computationally intensive verifiers can be reserved for higher levels of the arbitration graph.

The verification step is embedded as follows:
First, in \cref{algo:verifying_arbitrator:situation}, the current situation~$\situation$ is determined.
Based on this, the root arbitrator determines its best applicable and safe action using the \BestOption{$\situation$} function.
%
For this, it determines the set of options~$\applicableOptions \subseteq \options$ which are applicable in the present situation~$\situation$.
Like in the original arbitration algorithm, the options of an arbitrator can themselves be arbitrators or behavior components.
The applicable options~$\applicableOptions$ are then sorted into a descending list~$\sortedApplicableOptions$ according to the underlying strategy.
%
Now, for each option $a \in \sortedApplicableOptions$, it is checked whether its command $\command_a = \getCommand_a(\situation)$ withstands a verification~$\verifier (\command_a)$.
If so, this option is returned as the best applicable and safe option.
If none of the options passes the verification step, the arbitrator returns the error value $\text{NO\_SAFE\_OPTION}$.
%
Otherwise, the command~$\command$ returned by the root arbitrator can be executed and considered safe given the assumptions of the verifier~$\verifier$.

\subsection{\textbf{Mitigate} Unsafe \& Unreliable Behaviors}
If a behavior component fails the verification step, the arbitration graph must handle this situation to ensure the system remains safe and robust.
Even without additional measures, the verification step improves the safety of the system by preventing unsafe commands from being executed.
If an arbitrator detects a failure and has other applicable options, it simply chooses the next best option.
%
However, to further increase robustness and reduce performance degradation, we make use of the bottom-up approach of arbitration graphs to add fallback layers in the form of additional behavior components.

% Increase robustness by adding diverse or redundant behaviors.
One common approach in fault-tolerant systems is to increase the system's diversity and redundancy.
In the case of arbitration graphs this can be achieved by adding further behavior components.
A redundant behavior component is merely a duplicate instantiation of an existing behavior component.
If it is non-deterministic, the redundant behavior component might find a safe command where the original behavior component failed to do so.

In contrast, a diverse behavior component addresses the same task but with a different approach.
For example, a preferred experimental or learning-based behavior component could be complemented by a fallback that employs a more conservative yet stable method to generate the command.

% Increase robustness by adding continue last maneuver behavior
Another approach is to add a behavior component that repeats or continues the last command.
This helps to mitigate short-term failures such as a behavior component failing to produce a valid command for a single time step.

% Increase robustness by adding emergency behavior
Finally, a last resort behavior component should be added to the arbitration graph.
This behavior should be simple and safe, ensuring that the system always has a valid command to execute.
For example, in a mobile robot, the emergency behavior could be to stop moving and wait for further instructions.
Since this is the last resort, the emergency behavior does not need to pass the verification step.

% Seamless degradation
This layered approach using multiple fallback behavior components allows the performance of the system to degrade gracefully instead of having to execute an emergency command right away.

\subsection{Example: Safe Pac-Man}

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/pacman_scenario_cropped.png}
    \caption{A scenario where \behavior{EatClosestDot} fails to produce a valid command.}
    \label{fig:pacman-scenario}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\columnwidth,inkscapelatex=false]{figures/pacman_arbitrator_safe}
    \caption{The extended arbitration graph with fallback layers.
        The components highlighted in red were rejected by the verifier.
        The safety buoy indicates a last resort fallback which does not need to pass verification.}
    \label{fig:pacman-arbitrator-safe}
\end{figure}

While the original arbitration graph shown in \cref{fig:pacman-arbitrator-base} might have been sufficient under normal conditions, it might fail in practice if a behavior component returns an unsafe or unreliable command.
Consider the scenario depicted in \cref{fig:pacman-scenario}.
Since the ghosts are relatively far away and there is only one dot cluster left, the only applicable behavior is \behavior{EatClosestDot}.
Ideally, this behavior has the intended effect and Pac-Man should move towards the last remaining dots.
In our hypothetical scenario, however, a bug in the underlying path planning algorithm leads to erroneous results.
Consequently, the behavior component fails to generate a valid command, causing the system to behave unpredictably or even crash.

\cref{fig:pacman-arbitrator-safe} shows the arbitration graph from \cref{fig:pacman-arbitrator-base} extended with fallback layers.
With \behavior{EatClosestDot} not returning a valid command, the system falls back to the newly added \behavior{MoveRandomly} behavior component.
Moving Pac-Man in a random direction is a simple action which might help the primary behavior components to escape a deadlock and find a new valid command.
In our scenario, however, \behavior{MoveRandomly} returns a command which would lead to a collision with a wall.
Therefore, this command is rejected by the verifier as well, and the system falls back to the last resort behavior \behavior{StayInPlace}.
While it might be impossible to complete the level with this behavior, the system remains in a predictable and safe state giving the primary behavior components a chance to recover.

Of course, in this toy example it would be possible to find the bug in \behavior{EatClosestDot} through testing.
In a real-world scenario, however, it is not feasible to test all possible situations and bugs might only manifest in specific edge cases.
The proposed safety concept explicitly handles safety within the verifiers, transferring the responsibility away from individual behavior components and their arrangement to the verifiers.
This approach allows for the integration of imperfect or experimental behavior components without compromising the overall safety of the system.

