\section{Safe Arbitration Graphs}

Designing autonomous systems ideally involves ensuring that each behavior component operates reliably under all potential conditions.
However, achieving this level of reliability is often impractical, especially when deploying these systems in complex and dynamic environments.
Real-world applications might involve numerous behaviors, implemented by different teams, which increases the likelihood of bugs and inconsistencies.
Consequently, ensuring the overall safety and robustness of such systems presents a significant challenge.

The responsibility for safety in autonomous systems is a critical consideration.
Typically, the responsibility for individual behavior components lies with the developer who creates them.
This responsibility is manageable when components are simple and isolated. However, as systems grow in complexity, this approach becomes less scalable.
In traditional approaches such as behavior trees, the system engineer is tasked with integrating these behaviors into a coherent system, bearing the overall responsibility for system safety.

To address these challenges, we propose a safety concept embedded directly into the arbitration graph framework.
With this approach, we aim to reduce the burden on both behavior component developers and system engineers.
This approach allows for more scalable and maintainable safety assurance, ensuring that even in the presence of unreliable or unsafe behaviors, the system as a whole remains robust and secure.

In this section, we will explore methods for detecting and mitigating unsafe and unreliable behaviors, with the following safety goals in mind:

\begin{itemize}
    \item The system ensures a safety action is executed whenever the operating conditions exceed the system's designed capabilities.
    \item The system is robust against behavior component failures.
    \item The system prevents invalid control inputs.
    \item The system avoids risky control inputs.
\end{itemize}

\subsection{Detect Unsafe \& Unreliable Behaviors}
\begin{algorithm}
  \SetKwFunction{BestOption}{BestOption}%
  \SetKwProg{Fn}{function}{}{end}

  \Fn{\BestOption{situation $\situation$}}{\label{algo:verifying_arbitrator:bestOption}
    filter applicable options $\applicableOptions \subset \options$\;
    sort applicable options
      $\sortedApplicableOptions = \left< a_0, a_1, \dots \right> = \text{strategie}(\applicableOptions)$\;

    \For{$a \in \sortedApplicableOptions$}{
      get command $\command_a = \getCommand_a(\situation)$\;

      verify $\verification_a = \verifier (\command_a)$\;
      % \If(\tcp*[f]{Verifikation bestanden}){$\verification_a = 0$}{%
      \If{verification passed $\verification_a = 0$}{%
        \KwRet{$(\command_a, \verification_a)$}\;
      }
    }

    \KwRet{$(\emptyset, \text{NO\_SAFE\_OPTION})$}\;
    }
  \;
  \While{true}{
    determine current situation $\situation$\;\label{algo:verifying_arbitrator:situation}
    determine $\verifiedCommand = (\command, \verification) = $ \BestOption{$\situation$}\;
    \If{verification passed $\verification_a = 0$}{
      execute $\command$\;
    % }{
    %   Gebe Fehlermeldung mit $\verification_a$ aus\;
    }
  }
  \caption{Generic arbitration algorithm with verification \label{algo:verifying_arbitrator}}
\end{algorithm}

\Cref{algo:verifying_arbitrator} shows a generic arbitration algorithm extended with a verification step.
First, in \cref{algo:verifying_arbitrator:situation}, the current situation~$\situation$ is determined.
Based on this, the root arbitrator determines its best applicable and safe action using the \BestOption{$\situation$} function.
%
To do this, it filters from its options $\options$ (which may be a combination of behavior components and lower-level arbitrators) those options~$\applicableOptions$ that are applicable in the situation~$\situation$.
These are then sorted into a descending list~$\sortedApplicableOptions$ according to the underlying strategy.
%
Now, for each option $a \in \sortedApplicableOptions$, it is checked whether its command $\command_a = \getCommand_a(\situation)$ withstands a verification~$\verifier (\command_a)$.
If so, this option is returned as the best applicable and safe option.
If none of the options pass the verification step, the arbitrator returns the error value $\text{NO\_SAFE\_OPTION}$.
%
The command~$\command$ returned by the root arbitrator can then be executed and deemed safe given the assumptions of the verifier~$\verifier$.

The utilized verifier~$\verifier$ is domain-specific and may run various checks on the behavior command.
In Pac-Man for example, the verifier could check whether the command would lead to a collision with a wall or ghost.
In autonomous driving, we could verify if the behavior output is free of collisions and respects traffic rules.
A robotic manipulator could verify if the command is within the robot's workspace and does not exceed joint limits.
More generally, the verifier could check if the format of the command is correct and respects the system's specifications.

\subsection{Mitigate Unsafe \& Unreliable Behaviors}
If a behavior component fails the verification step, the arbitration graph must handle this situation to ensure the system remains safe and robust.
Even without additional measures, the verification step already improves the system's safety by preventing unsafe commands from being executed.
If an arbitrator detects a failure but has other applicable options, it simply chooses the next best option.

However, to further increase robustness, we make use of the bottom-up approach of arbitration graphs to add fallback layers in form of additional behavior components.

% Increase robustness by adding diverse or redundant behaviors.
One common approach in fault-tolerant systems is to increase the system's diversity and redundancy.
In the case of arbitration graphs this can be achieved by adding additional behavior components.
A redundant behavior component is simply a duplicate instantiation of an existing behavior component.
If the behavior component is non-deterministic, the redundant behavior component might find a safe command where the original behavior component failed.
A diverse behavior component solves the same task but with a different approach.
For example, an experimental or learning-based behavior could be complemented by a more conservative, but stable and reliable behavior component.
 
% Increase robustness by adding continue last maneuver behavior
Another approach is to add a behavior component that repeats or continues the last maneuver.
This helps to mitigate short-term failures, such as a single frame where the behavior component fails to produce a valid command.

% Increase robustness by adding emergency behavior
Finally, a last resort behavior component.
This behavior should be simple and safe, ensuring that the system always has a valid command to execute.
For example, in a mobile robot, the emergency behavior could be to stop moving and wait for further instructions.
Since this is the last resort, the emergency behavior does not need to pass the verification step.

% Seamless degradation
This layered approach using multiple fallback allows the system's performance to degrade gracefully instead of having to execute an emergency command immediately.

\subsection{Example: Safe Pac-Man}

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/pacman_scenario_cropped.png}
    \caption{A scenario where the \behavior{EatClosestDot} behavior fails to produce a valid command.}
    \label{fig:pacman-scenario}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\columnwidth,inkscapelatex=false]{figures/pacman_arbitrator_safe}
    \caption{The extended arbitration graph with fallback layers. The icons indicate whether a behavior component
        timed out (hourglass),
        failed verification (lightning bolt),
        or is a last resort fallback (safety buoy).}
    \label{fig:pacman-arbitrator-safe}
\end{figure}

Where the original arbitration graph shown in \cref{fig:entt-pacman} may have been functional in principle, it might fail in practice if a behavior component returns an unsafe or unreliable command.
Consider the scenario depicted in \cref{fig:pacman-scenario}.
Since no ghosts are nearby and there is only one dot cluster left, the only applicable behavior is \behavior{EatClosestDot}.
Ideally, this behavior has the intended effect and Pac-Man should move towards the last remaining dots.
If, however, the behavior component fails to produce a valid command, the system might behave unpredictably or even crash.
For example, a bug in the underlying path planning algorithm might get stuck in a loop.

\todo{Add a figure showing the arbitration graph with fallback layers} shows the arbitration graph extended with fallback layers.
With the \behavior{EatClosestDot} not returning a command in a reasonable time, the system falls back to the newly added \behavior{RandomWalk} behavior.
This behavior component simply moves Pac-Man in a random direction.
This random exploration might help to escape a deadlock and find a new valid command.
In our scenario, however, the \behavior{RandomWalk} return a command which would lead to a collision with a wall.
This command is therefore rejected by the verifier, and the system falls back to the last resort behavior \behavior{StayInPlace}.
While it might be impossible to complete the level with this behavior, the system remains in a predictable and safe state giving the primary behavior components a chance to recover.

Of course it would be possible to find the bug in \behavior{EatClosestDot} in a test before deployment in this toy-example.
In a real-world scenario, however, it is impossible to test all possible situations and bugs might only occur in specific edge cases.
The proposed safety concept centralizes the responsibility for safety in the verifier, which enables the usage of imperfect or experimental behavior components without risking the system's overall safety.

