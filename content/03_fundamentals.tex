% !TeX root = ../root.tex
% -*- root: ../root.tex -*-

\section{Fundamentals}

\todo[inline]{Fix references}

\subsection{Decision-Making}

Decision-making is crucial in a robot's planning module, determining commands based on the current situation.
Methods range from graph-based techniques like A*, PRM*, and RRT* to probabilistic methods and machine learning for implicit decisions.
This work, however, focuses on rule-based methods, such as finite state machines, behavior trees, or arbitration graphs, which address decision making through discrete state or mode transitions.

% Finite State Machines
FSMs, from hardware design and theoretical computer science, represent behavior modes with transitions triggered by events. Despite their simplicity, FSMs scale poorly and are difficult to modify due to extensive transitions.

% Behavior Trees
Behavior trees, initially designed for game development, have been increasingly used in robotics since 2012.
They separate behavior decision-making from execution, forming loop-free undirected graphs.
Internal nodes determine selection mechanisms, while leaves describe behaviors and conditions.
Evaluated at a fixed frequency, nodes return their status as running, completed, or failed.
Control flow nodes decide on further evaluations.

Condition nodes check if their underlying conditions are met without affecting the environment, while action nodes execute behaviors and return their status.
By distinguishing between condition and action nodes, the preconditions of a behavior are decoupled from the actual execution of the behavior.
To design a safe system, these actions must be linked to reliable condition nodes.

Behavior trees generalize many architectures, such as hierarchical finite state machines and decision trees [Col17], excelling in modularity, hierarchical organization, reusability, responsiveness, and interpretability [Col18].
Their flexibility allows reuse of individual behaviors without specifying their relations [Bag12].
The selection mechanism is intuitive and easy to follow during operation.
However, extensive preconditions can make representations cumbersome, and safety as well as reliability depend significantly on node arrangement.
These drawbacks are addressed by arbitration graphs.

% Arbitration Graphs
The concept of behavior arbitration originated in the context of robot soccer, integrating ideas from Brooks' behavior-based subsumption, knowledge-based architectures like Belief-Desire-Intention (BDI), and programming paradigms such as object-oriented programming.
This approach was thoroughly described in [Lau10] and is summarized in the following.

This modular framework is characterized by clear interfaces for transparent decision-making, using atomic behavior modules to represent simple abilities and behaviors.
These modules are combined using arbitrators to create complex system behaviors.

The input to a behavior module is the current situation $\situation$, provided as sensor data or an interpreted environment model.
If its preconditions are met, the invocation condition $\invCond$ indicates that the behavior is applicable in the current situation $\situation$.
In this case, the higher-level instance (i.e., the arbitrator) can instruct the behavior module to compute a command $\command_a$.
The currently active behavior module additionally uses the commitment condition $\comCond$ to indicate that its behavior can be continued.
Consequently, the calling instance does not need to know the prerequisites for executing the behavior's command.

Generic arbitrators combine behavior modules $\options = \left< o_0, o_1, \dots \right>$, 
filter out the applicable subset $\applicableOptions \subset \options$ using their invocation and commitment conditions, 
and select the best applicable option $a^*$ for execution.
Arbitration schemes include priority-based, sequence-based, cost-based, and random.
Due to inheritance and a shared interface, arbitrators can include both behavior modules and other arbitrators, enabling a hierarchical architecture.

\subsection{Fault-tolerant/Robust Systems}

In automated systems, both hardware and software issues can compromise performance and safety.
Causes include programming errors and runtime issues such as optimization problems, making error diagnosis and treatment crucial in system design and during runtime.

Research on reliable and fault-tolerant systems aims to design dependable hardware and software despite potential errors, using metrics like error probability, mean lifespan, failure rate, and availability.
Terminology varies, but disturbances ("faults") can lead to errors, potentially causing system failures.

Reliability measures include error prevention, removal, tolerance, and prediction.
Prevention and removal focus on design and development, while tolerance involves detecting and preventing operational errors.
Prediction estimates future failures.
Error tolerance involves diagnosing and handling errorsâ€”restoring faulty components, computing correct results despite faults, or removing faulty components from the system.

